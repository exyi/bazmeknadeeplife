{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7120083-c7f7-4fbc-a718-29ed1edefe09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:01:19.060734Z",
     "start_time": "2024-06-01T08:01:19.048876Z"
    }
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import muon as mu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mofax as mofa\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.infer import SVI, Trace_ELBO, autoguide\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import softplus\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import seaborn as sns\n",
    "import muon as mu\n",
    "import anndata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def to_device(t): return torch.tensor(t).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e0d5bd-c307-4b4d-9b9d-ef46a02dc580",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:01:25.387204Z",
     "start_time": "2024-06-01T08:01:20.058306Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/venvs/deeplife2/lib/python3.12/site-packages/anndata/_core/anndata.py:1820: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "# dir=\"/scratch/deeplife/\"\n",
    "dir=\"data/\"\n",
    "pbmc = sc.read_10x_h5(dir+\"5k_pbmc_protein_v3_nextgem_filtered_feature_bc_matrix.h5\", gex_only=False)\n",
    "pbmc.var_names_make_unique()\n",
    "pbmc.layers[\"counts\"] = pbmc.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c95a25a-bc4d-43ca-9a2e-bb4f45d1ed1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:01:26.223901Z",
     "start_time": "2024-06-01T08:01:25.368144Z"
    }
   },
   "outputs": [],
   "source": [
    "protein = pbmc[:, pbmc.var[\"feature_types\"] == \"Antibody Capture\"].copy()\n",
    "rna = pbmc[:, pbmc.var[\"feature_types\"] == \"Gene Expression\"].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "215ca913ab3c58b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:01:26.294524Z",
     "start_time": "2024-06-01T08:01:26.207710Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MOFA(PyroModule):\n",
    "    def __init__(self, Ys: dict[str, torch.Tensor], K, batch_size=128, num_iterations=4000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        self.K = K  # number of factors \n",
    "\n",
    "        self.obs_masks = {k: torch.logical_not(torch.isnan(Y)) for k, Y in Ys.items()}\n",
    "        # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "        self.Ys = {k: torch.nan_to_num(Y, nan=0) for k, Y in Ys.items()}  # data/observations\n",
    "        \n",
    "        # assert sample dim same in Ys\n",
    "        num_samples = set(Y.shape[0] for Y in self.Ys.values())\n",
    "        assert len(num_samples) == 1\n",
    "        self.num_samples = next(iter(num_samples))\n",
    "        self.num_features = {k: v.shape[1] for k, v in self.Ys.items()}\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K) \n",
    "        \n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\" Creates the model.\n",
    "        \n",
    "        The model needs to be created repeatedly (not sure why), in any case, it is important now, when using \n",
    "        `subsample_size` batch size to subsample the dataset differently in each train iteration\n",
    "        \"\"\"\n",
    "        \n",
    "        # needs to be shared, so returns the same indices in one train step\n",
    "        sample_plate = pyro.plate(\"sample\", self.num_samples, subsample_size=self.batch_size)\n",
    "        # the plates get assigned a dim, depending on when in the plate hierarchy they are used. Unfortunately we want to use\n",
    "        #   feature plates once outside and once inside other plate (sample resp. latent_factor plates, see below)\n",
    "        #   we therefore need to create separate plates for each of those usages\n",
    "        get_feature_plates = lambda dim: {k: pyro.plate(f\"feature_{k}_{dim}\", num_feats) for k, num_feats in self.num_features.items()}\n",
    "\n",
    "        # W matrices for each modality\n",
    "        Ws = {}\n",
    "    \n",
    "        # for each modality create W matrix and alpha vectors\n",
    "        for m, feature_plate in get_feature_plates(-2).items():\n",
    "            # the actual dimensions obtained by plates are read from right to left/inner to outer\n",
    "            with self.latent_factor_plate:\n",
    "                # Sample alphas (controls narrowness of weight distr for each factor) from a Gamma distribution\n",
    "                # Gamma parametrization k, theta or eq. a, b; (where k=a and theta=1/b) \n",
    "                # (if k integer) Gamma = the sum of k independent exponentially distributed random variables, each of \n",
    "                # which has a mean of theta\n",
    "                alpha = pyro.sample(f\"alpha_{m}\", pyro.distributions.Gamma(to_device(1.), 1.))\n",
    "                \n",
    "                with feature_plate:\n",
    "                    # sample weight matrix with Normal prior distribution with alpha narrowness\n",
    "                    Ws[m] = pyro.sample(f\"W_{m}\", pyro.distributions.Normal(to_device(0.), 1. / alpha))                \n",
    "                \n",
    "        # create Z matrix\n",
    "        # (the actual dimensions are read from right to left/inner to outer)\n",
    "        with self.latent_factor_plate, sample_plate:\n",
    "            # sample factor matrix with Normal prior distribution\n",
    "            Z = pyro.sample(\"Z\", pyro.distributions.Normal(to_device(0.), 1.))\n",
    "    \n",
    "        # estimate for Y\n",
    "        Y_hats = {k: torch.matmul(Z, W.t()) for k, W in Ws.items()}\n",
    "        \n",
    "        for m, feature_plate in get_feature_plates(-1).items():\n",
    "            with feature_plate:\n",
    "                # sample scale (tau) parameter for each feature-~~sample~~ pair with LogNormal prior (has to be positive)\n",
    "                # add 0.001 to avoid NaNs when Normal(Ïƒ = 0)\n",
    "                scale_tau = 0.001 + pyro.sample(f\"scale_{m}\", pyro.distributions.LogNormal(to_device(0.), 1.))\n",
    "                \n",
    "                with sample_plate as sub_indices:\n",
    "                    Y, Y_hat = self.Ys[m][sub_indices, :], Y_hats[m]\n",
    "                    \n",
    "                    # masking the NA values such that they are not considered in the distributions\n",
    "                    obs_mask = self.obs_masks[m][sub_indices, :]                    \n",
    "                    with pyro.poutine.mask(mask=obs_mask):\n",
    "                        # # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                        # scale = pyro.sample(\"scale\", pyro.distributions.LogNormal(0., 1.))\n",
    "                        \n",
    "                        # compare sampled estimation to the true observation Y\n",
    "                        pyro.sample(f\"obs_{m}\", pyro.distributions.Normal(Y_hat, scale_tau), obs=Y)\n",
    "                        # pyro.sample(\"obs\", pyro.distributions.NegativeBinomial(Y_hat, scale_tau), obs=Y)\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.002})\n",
    "        elbo = Trace_ELBO()\n",
    "        guide = autoguide.AutoDelta(self.model)\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        train_loss = []\n",
    "        for j in range(self.num_iterations):\n",
    "            # calculate the loss and take a gradient step\n",
    "            # (loss should be already scaled down by the subsample_size)\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.num_samples)\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.num_samples))\n",
    "        \n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        # map_estimates = guide(self.Y)  # not sure why needed Y?\n",
    "        # \"Note that Pyro enforces that model() and guide() have the same call signature, i.e. both callables should take the same arguments.\"\n",
    "        with torch.no_grad():\n",
    "            map_estimates = guide()\n",
    "        \n",
    "        return train_loss, map_estimates, guide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e54a58-6c6b-4db7-9bc5-9cdf9b49ed16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b384fdc-4864-44e3-bffd-c7ca858d52a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:46:53.288286Z",
     "start_time": "2024-06-01T07:40:36.738842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 2271781.8510\n",
      "[iteration 0201] loss: 1406116.4590\n",
      "[iteration 0401] loss: 918445.2070\n",
      "[iteration 0601] loss: 625384.0857\n",
      "[iteration 0801] loss: 335243.3495\n",
      "[iteration 1001] loss: 300688.9161\n",
      "[iteration 1201] loss: 271672.2342\n",
      "[iteration 1401] loss: 163889.6779\n",
      "[iteration 1601] loss: 107501.3321\n",
      "[iteration 1801] loss: 93330.2243\n",
      "[iteration 2001] loss: 75623.1707\n",
      "[iteration 2201] loss: 55720.1404\n",
      "[iteration 2401] loss: 21617.3854\n",
      "[iteration 2601] loss: 17825.7569\n",
      "[iteration 2801] loss: -1371.1647\n",
      "[iteration 3001] loss: -23624.2083\n",
      "[iteration 3201] loss: -27397.4940\n",
      "[iteration 3401] loss: -27975.6374\n",
      "[iteration 3601] loss: -34453.2147\n",
      "[iteration 3801] loss: -40593.5731\n",
      "[iteration 4001] loss: -50273.8074\n",
      "[iteration 4201] loss: -54369.6320\n",
      "[iteration 4401] loss: -55900.9729\n",
      "[iteration 4601] loss: -56016.3566\n",
      "[iteration 4801] loss: -65426.0103\n",
      "[iteration 5001] loss: -66230.6058\n",
      "[iteration 5201] loss: -70710.3724\n",
      "[iteration 5401] loss: -68857.9214\n",
      "[iteration 5601] loss: -70122.0832\n",
      "[iteration 5801] loss: -75795.3808\n",
      "[iteration 6001] loss: -77861.6909\n",
      "[iteration 6201] loss: -74470.8465\n",
      "[iteration 6401] loss: -78798.2909\n",
      "[iteration 6601] loss: -77244.3219\n",
      "[iteration 6801] loss: -77483.3971\n",
      "[iteration 7001] loss: -56388.4093\n",
      "[iteration 7201] loss: -81316.2723\n",
      "[iteration 7401] loss: -80745.1836\n",
      "[iteration 7601] loss: -78347.0238\n",
      "[iteration 7801] loss: -81174.5889\n",
      "[iteration 8001] loss: -81255.8010\n",
      "[iteration 8201] loss: -82666.5718\n",
      "[iteration 8401] loss: -82825.4539\n",
      "[iteration 8601] loss: -83156.8305\n",
      "[iteration 8801] loss: -81870.1278\n",
      "[iteration 9001] loss: -83415.9311\n",
      "[iteration 9201] loss: -84438.3162\n",
      "[iteration 9401] loss: -75599.5241\n",
      "[iteration 9601] loss: -82866.2051\n",
      "[iteration 9801] loss: -84245.0734\n",
      "[iteration 10001] loss: -83651.8819\n",
      "[iteration 10201] loss: -85137.8168\n",
      "[iteration 10401] loss: -83233.1275\n",
      "[iteration 10601] loss: -85705.7552\n",
      "[iteration 10801] loss: -83793.1029\n",
      "[iteration 11001] loss: -86089.1481\n",
      "[iteration 11201] loss: -86370.9417\n",
      "[iteration 11401] loss: -85517.9028\n",
      "[iteration 11601] loss: -85059.8539\n",
      "[iteration 11801] loss: -86825.5050\n",
      "[iteration 12001] loss: -85626.1332\n",
      "[iteration 12201] loss: -86714.9580\n",
      "[iteration 12401] loss: -86028.5758\n",
      "[iteration 12601] loss: -86803.8722\n",
      "[iteration 12801] loss: -86975.7764\n",
      "[iteration 13001] loss: -85937.6006\n",
      "[iteration 13201] loss: -85746.0342\n",
      "[iteration 13401] loss: -86028.4055\n",
      "[iteration 13601] loss: -85515.2243\n",
      "[iteration 13801] loss: -84800.6870\n",
      "[iteration 14001] loss: -87081.8765\n",
      "[iteration 14201] loss: -85860.0414\n",
      "[iteration 14401] loss: -85759.7807\n",
      "[iteration 14601] loss: -86814.0621\n",
      "[iteration 14801] loss: -87048.3214\n",
      "[iteration 15001] loss: -85705.2505\n",
      "[iteration 15201] loss: -85670.0572\n",
      "[iteration 15401] loss: -85423.6696\n",
      "[iteration 15601] loss: -83838.3804\n",
      "[iteration 15801] loss: -85849.3065\n",
      "[iteration 16001] loss: -84330.5569\n",
      "[iteration 16201] loss: -83711.1822\n",
      "[iteration 16401] loss: -86369.2493\n",
      "[iteration 16601] loss: -84804.0138\n",
      "[iteration 16801] loss: -85834.5559\n",
      "[iteration 17001] loss: -84725.9346\n",
      "[iteration 17201] loss: -85362.2249\n",
      "[iteration 17401] loss: -86255.0496\n",
      "[iteration 17601] loss: -87030.0436\n",
      "[iteration 17801] loss: -84059.6485\n",
      "[iteration 18001] loss: -84251.3187\n",
      "[iteration 18201] loss: -86223.8985\n",
      "[iteration 18401] loss: -87503.2452\n",
      "[iteration 18601] loss: -84620.7917\n",
      "[iteration 18801] loss: -85805.2750\n",
      "[iteration 19001] loss: -85479.3518\n",
      "[iteration 19201] loss: -86927.5657\n",
      "[iteration 19401] loss: -87265.6816\n",
      "[iteration 19601] loss: -84834.3041\n",
      "[iteration 19801] loss: -86762.5445\n",
      "[iteration 20001] loss: -85032.9245\n",
      "[iteration 20201] loss: -84408.0715\n",
      "[iteration 20401] loss: -85711.1782\n",
      "[iteration 20601] loss: -85553.6854\n",
      "[iteration 20801] loss: -85450.7052\n",
      "[iteration 21001] loss: -85814.7413\n",
      "[iteration 21201] loss: -85167.5742\n",
      "[iteration 21401] loss: -84908.4906\n",
      "[iteration 21601] loss: -86048.8851\n",
      "[iteration 21801] loss: -86698.9649\n",
      "[iteration 22001] loss: -85062.0142\n",
      "[iteration 22201] loss: -86868.7311\n",
      "[iteration 22401] loss: -83880.8147\n",
      "[iteration 22601] loss: -85978.1823\n",
      "[iteration 22801] loss: -85691.0586\n",
      "[iteration 23001] loss: -83920.2076\n",
      "[iteration 23201] loss: -85986.5070\n",
      "[iteration 23401] loss: -84449.0792\n",
      "[iteration 23601] loss: -84870.5007\n",
      "[iteration 23801] loss: -85844.0855\n",
      "[iteration 24001] loss: -82792.8004\n",
      "[iteration 24201] loss: -85689.2642\n",
      "[iteration 24401] loss: -86255.7093\n",
      "[iteration 24601] loss: -85425.9025\n",
      "[iteration 24801] loss: -85439.7299\n",
      "[iteration 25001] loss: -85594.1614\n",
      "[iteration 25201] loss: -85736.2216\n",
      "[iteration 25401] loss: -86541.4475\n",
      "[iteration 25601] loss: -85319.8678\n",
      "[iteration 25801] loss: -85377.2179\n",
      "[iteration 26001] loss: -83986.4618\n",
      "[iteration 26201] loss: -84267.7831\n",
      "[iteration 26401] loss: -86128.3561\n",
      "[iteration 26601] loss: -85026.0656\n",
      "[iteration 26801] loss: -85541.6301\n",
      "[iteration 27001] loss: -86314.5579\n",
      "[iteration 27201] loss: -86716.6239\n",
      "[iteration 27401] loss: -85365.0570\n",
      "[iteration 27601] loss: -86784.1034\n",
      "[iteration 27801] loss: -86321.6054\n",
      "[iteration 28001] loss: -86041.6111\n",
      "[iteration 28201] loss: -86329.7740\n",
      "[iteration 28401] loss: -86160.6597\n",
      "[iteration 28601] loss: -85390.7565\n",
      "[iteration 28801] loss: -86579.9219\n",
      "[iteration 29001] loss: -84902.5605\n",
      "[iteration 29201] loss: -86638.4329\n",
      "[iteration 29401] loss: -86295.0570\n",
      "[iteration 29601] loss: -85587.4521\n",
      "[iteration 29801] loss: -86810.7477\n",
      "[iteration 30001] loss: -85313.2710\n",
      "[iteration 30201] loss: -85972.3783\n",
      "[iteration 30401] loss: -85644.7119\n",
      "[iteration 30601] loss: -87211.4276\n",
      "[iteration 30801] loss: -85863.7785\n",
      "[iteration 31001] loss: -86911.5794\n",
      "[iteration 31201] loss: -83830.4394\n",
      "[iteration 31401] loss: -86641.7933\n",
      "[iteration 31601] loss: -85290.3120\n",
      "[iteration 31801] loss: -86364.1550\n",
      "[iteration 32001] loss: -84827.9403\n",
      "[iteration 32201] loss: -85172.6820\n",
      "[iteration 32401] loss: -84602.1966\n",
      "[iteration 32601] loss: -85523.6976\n",
      "[iteration 32801] loss: -87217.1528\n",
      "[iteration 33001] loss: -86703.7326\n",
      "[iteration 33201] loss: -85649.6623\n",
      "[iteration 33401] loss: -85716.6878\n",
      "[iteration 33601] loss: -84578.0826\n",
      "[iteration 33801] loss: -86281.6053\n",
      "[iteration 34001] loss: -86961.7798\n",
      "[iteration 34201] loss: -86228.7189\n",
      "[iteration 34401] loss: -85634.4940\n",
      "[iteration 34601] loss: -85832.9472\n",
      "[iteration 34801] loss: -84514.7591\n",
      "[iteration 35001] loss: -85891.2287\n",
      "[iteration 35201] loss: -86801.9711\n",
      "[iteration 35401] loss: -83996.0407\n",
      "[iteration 35601] loss: -85920.4089\n",
      "[iteration 35801] loss: -85898.7379\n",
      "[iteration 36001] loss: -86662.9202\n",
      "[iteration 36201] loss: -86556.9368\n",
      "[iteration 36401] loss: -86820.2921\n",
      "[iteration 36601] loss: -85104.7817\n",
      "[iteration 36801] loss: -84710.8903\n",
      "[iteration 37001] loss: -85570.6513\n",
      "[iteration 37201] loss: -87157.6495\n",
      "[iteration 37401] loss: -86883.1001\n",
      "[iteration 37601] loss: -85787.0965\n",
      "[iteration 37801] loss: -85950.6452\n",
      "[iteration 38001] loss: -87167.7252\n",
      "[iteration 38201] loss: -87033.3303\n",
      "[iteration 38401] loss: -86635.7735\n",
      "[iteration 38601] loss: -86013.6711\n",
      "[iteration 38801] loss: -86817.4439\n",
      "[iteration 39001] loss: -86712.7057\n",
      "[iteration 39201] loss: -84988.7671\n",
      "[iteration 39401] loss: -85702.6790\n",
      "[iteration 39601] loss: -87450.1364\n",
      "[iteration 39801] loss: -86827.8637\n"
     ]
    }
   ],
   "source": [
    "mofa = MOFA({\n",
    "    'rna': torch.tensor(rna.X.toarray(), device=device),\n",
    "    'protein': torch.tensor(protein.X.toarray(), device=device),\n",
    "}, K=5, batch_size=128, num_iterations=40000)\n",
    "loss, map_estimates, trained_guide = mofa.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10f723678a643d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:46:53.317610Z",
     "start_time": "2024-06-01T07:46:53.297148Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha_rna': tensor([ 0.6805,  0.7047, 51.2332,  6.3386, 44.6454],\n",
       "        grad_fn=<ExpandBackward0>),\n",
       " 'W_rna': tensor([[ 5.7743e-16,  4.8005e-16,  2.3599e-16,  2.2752e-17,  1.7289e-15],\n",
       "         [ 1.5914e-04,  2.6798e-04,  1.2638e-03,  1.9229e-03,  1.1706e-03],\n",
       "         [-7.3769e-06, -6.1725e-06,  9.5526e-06, -1.1013e-05,  1.0729e-05],\n",
       "         ...,\n",
       "         [-1.4335e-01,  5.1932e-02, -1.5062e-02,  6.9615e-02,  2.6003e-02],\n",
       "         [ 7.9154e-22, -3.3275e-22, -7.4161e-23, -4.1880e-22,  1.9829e-22],\n",
       "         [-1.4943e-03, -2.7019e-03, -7.7028e-03, -8.6110e-04,  1.1006e-02]],\n",
       "        grad_fn=<ExpandBackward0>),\n",
       " 'alpha_protein': tensor([0.3126, 0.5185, 1.0603, 0.4464, 0.7072], grad_fn=<ExpandBackward0>),\n",
       " 'W_protein': tensor([[-2.5605, -0.4950, -0.4921, -2.5739,  0.4239],\n",
       "         [-4.4372, -1.8422,  0.8739, -2.3409,  1.4852],\n",
       "         [-2.8029, -0.1755, -0.7576, -1.4559,  0.1247],\n",
       "         [-1.8529, -0.6747,  0.1978, -2.7917,  2.1802],\n",
       "         [-2.6411, -0.4595, -0.2467, -2.2207,  1.6468],\n",
       "         [-2.0921, -0.3927, -1.3260, -2.6907,  1.4632],\n",
       "         [-2.4458, -0.3391, -0.6757, -1.8404,  1.4905],\n",
       "         [-2.5404, -0.1281, -0.7828, -1.2156,  0.4404],\n",
       "         [-3.1837,  0.0571, -1.9587, -2.0602,  1.7360],\n",
       "         [-3.7173, -0.3593, -1.3647, -2.4394,  0.8677],\n",
       "         [-3.1601, -0.9921,  0.5236, -1.9675,  1.8245],\n",
       "         [-3.9676, -0.6616, -0.5271, -1.8879,  2.1845],\n",
       "         [-0.7695,  0.4669, -0.0296,  0.2245, -0.1449],\n",
       "         [-1.7902, -0.5411, -0.1056, -1.8654,  0.8266],\n",
       "         [-2.9189, -0.4928, -0.8341, -2.6982,  2.4374],\n",
       "         [-2.0873,  0.7162, -0.4299, -1.3707,  1.3460],\n",
       "         [-4.2247, -0.2061, -1.6288, -3.5030,  1.2944],\n",
       "         [-3.9960,  0.4592,  0.4770, -2.5317,  1.1247],\n",
       "         [-3.9411,  4.6112, -0.5715, -1.9992, -0.4473],\n",
       "         [-2.9915, -1.6268, -0.6684, -2.4151,  1.7146],\n",
       "         [-3.3914, -0.2744, -0.4954, -2.1764,  2.4196],\n",
       "         [-4.3946,  4.8948, -1.1573, -1.9360, -0.2797],\n",
       "         [-2.8421,  0.8616, -1.1415, -2.7973,  0.8898],\n",
       "         [-3.0801,  2.5060, -1.1540, -1.8546,  0.2760],\n",
       "         [-4.6037, -0.0216, -1.0194, -3.4494,  2.1335],\n",
       "         [-2.9171,  2.5548, -1.0037, -1.8388,  1.3215],\n",
       "         [-2.4407, -0.5446, -1.2216, -2.3167,  1.1079],\n",
       "         [-2.9687, -0.9128, -0.8766, -3.0775,  2.6734],\n",
       "         [-4.7278,  5.1176, -0.6608, -1.6910, -0.3320],\n",
       "         [-2.5713,  2.4029, -0.9100, -1.3361, -0.0510],\n",
       "         [-3.1400,  2.8433, -0.9514, -1.3096, -0.3444],\n",
       "         [-2.5988,  2.6037, -1.2342, -2.0546,  0.1759]],\n",
       "        grad_fn=<ExpandBackward0>),\n",
       " 'Z': tensor([[-1.0713e-01,  7.0127e-03,  7.6328e-02, -1.6219e-01,  1.1213e-01],\n",
       "         [ 1.8984e-01,  2.4890e-01, -1.7269e-01,  2.1753e-01, -2.4389e-01],\n",
       "         [ 2.1901e-02,  7.2665e-02, -2.9037e-02,  8.1809e-02, -6.7625e-02],\n",
       "         [ 3.9313e-02,  7.7193e-02, -2.8005e-02, -6.0252e-02,  3.2980e-02],\n",
       "         [-3.9871e-02,  2.6358e-02, -2.7851e-02,  2.4240e-03, -9.9758e-03],\n",
       "         [-1.8107e-01,  5.0863e-02, -3.6199e-02,  3.6990e-02, -3.2992e-02],\n",
       "         [-1.6250e-01,  6.7188e-02,  4.2626e-02, -6.5415e-02,  5.9783e-02],\n",
       "         [-1.3452e-01, -1.6851e-03,  9.3020e-02,  8.0518e-02, -7.1117e-02],\n",
       "         [-7.7569e-02,  4.7234e-02,  1.8620e-02, -5.1026e-03,  2.5814e-02],\n",
       "         [-2.0534e-01, -1.6958e-01,  2.0977e-02,  2.8226e-02,  1.1225e-02],\n",
       "         [ 8.6922e-02,  5.3467e-01, -1.6903e-01,  1.9153e-01, -1.5043e-01],\n",
       "         [-4.6033e-02,  3.7654e-02,  3.4053e-03,  6.1790e-04,  7.3261e-03],\n",
       "         [-2.0224e-02,  5.4566e-02, -1.1273e-02, -9.6625e-03, -2.7974e-03],\n",
       "         [ 8.2636e-04,  2.7204e-02, -7.0535e-03, -4.4445e-03, -4.3156e-03],\n",
       "         [ 4.1086e-02,  2.9203e-02, -3.7460e-02, -6.0198e-02,  2.9683e-02],\n",
       "         [-3.6360e-02,  2.0442e-02, -6.0192e-03,  6.5256e-03, -1.0430e-02],\n",
       "         [ 1.2713e-01,  3.4352e-01, -3.6936e-01,  4.4213e-01, -3.2586e-01],\n",
       "         [-8.5512e-02,  7.7268e-02,  1.3225e-03, -1.4451e-02, -9.0200e-05],\n",
       "         [-6.1856e-03, -1.6718e-02,  2.5056e-03,  1.9756e-03, -4.1001e-03],\n",
       "         [-9.3174e-02,  7.1235e-02, -1.6815e-02, -1.4621e-02, -1.9871e-02],\n",
       "         [-4.6205e-02,  6.6643e-02, -2.0985e-02, -5.3374e-03,  2.6812e-02],\n",
       "         [ 1.3365e-01,  2.4942e-01, -2.9830e-01,  2.0188e-01, -1.9728e-01],\n",
       "         [-6.1340e-02,  5.8990e-02,  6.8738e-03, -6.9144e-04,  1.2808e-02],\n",
       "         [ 1.3228e-01, -1.3408e-01, -5.1189e-03, -2.5643e-02,  1.6193e-02],\n",
       "         [-1.3678e-01, -2.8357e-03,  2.8588e-02, -1.6806e-02,  3.2416e-02],\n",
       "         [-1.2781e-01,  8.9258e-02,  3.7773e-02, -4.9817e-02,  1.0324e-01],\n",
       "         [ 7.0845e-04,  8.1801e-04, -2.6733e-03, -1.5329e-03,  9.8063e-04],\n",
       "         [-6.2881e-02,  2.1011e-02, -3.7770e-02,  7.2566e-03, -3.2010e-03],\n",
       "         [-3.5484e-02,  2.1893e-02, -1.3922e-03,  7.9281e-03, -2.5601e-03],\n",
       "         [-1.1817e-01,  9.9977e-02, -2.2104e-02,  1.7716e-02, -7.5106e-03],\n",
       "         [-1.5267e-02,  7.0167e-02, -2.6893e-02,  1.0071e-01, -5.4947e-02],\n",
       "         [-1.4033e-01,  2.9534e-01, -4.3915e-02, -4.6150e-03, -7.2845e-02],\n",
       "         [ 9.0574e-02,  2.9819e-01, -5.2917e-02,  3.3897e-02, -3.3843e-02]],\n",
       "        grad_fn=<ExpandBackward0>),\n",
       " 'scale_rna': tensor([0.0024, 0.0040, 0.0006,  ..., 0.1521, 0.0008, 0.0173],\n",
       "        grad_fn=<ExpandBackward0>),\n",
       " 'scale_protein': tensor([61.4854, 72.2264, 58.2279, 63.9345, 81.1319, 28.2433, 33.2099, 89.7395,\n",
       "         23.4209, 24.1401, 52.3762, 74.1119,  0.4807, 82.4488, 49.1028, 68.6456,\n",
       "         49.4346, 30.7254,  5.9522, 49.9956, 37.7241,  8.7760, 22.8026,  9.9223,\n",
       "         59.2376, 26.1515, 21.3816, 41.0663, 16.2987, 10.5838, 10.1110, 10.3787],\n",
       "        grad_fn=<ExpandBackward0>)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee25ae3f-258d-4c34-b081-e22b7df0594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mofax, h5py, os\n",
    "\n",
    "os.remove('models/pyro-MOFA.h5')\n",
    "\n",
    "with h5py.File('models/pyro-MOFA.h5', 'w') as f:\n",
    "    f['features/rna'] = np.array([*rna.var['feature_types'].index], dtype='S')\n",
    "    f['features/protein'] = np.array([*protein.var['feature_types'].index], dtype='S')\n",
    "    f['samples/group1'] = np.array([*pbmc.obs.index], dtype='S')\n",
    "    f['data/rna/group1'] = rna.X.toarray()\n",
    "    f['data/protein/group1'] = protein.X.toarray()\n",
    "    f['expectations/Z/group1'] = map_estimates['Z'].detach().cpu().numpy()\n",
    "    f['expectations/W/rna'] = map_estimates['W_rna'].detach().cpu().numpy()\n",
    "    f['expectations/W/protein'] = map_estimates['W_protein'].detach().cpu().numpy()\n",
    "    f['model_options/likelihoods'] = np.array(['gaussian', 'gaussian'], dtype='S')\n",
    "\n",
    "\n",
    "mofax_model = mofax.mofa_model('models/pyro-MOFA.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplife2)",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
