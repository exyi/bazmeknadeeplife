{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7120083-c7f7-4fbc-a718-29ed1edefe09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:01:19.060734Z",
     "start_time": "2024-06-01T08:01:19.048876Z"
    }
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import muon as mu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mofax as mofa\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro\n",
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.infer import SVI, Trace_ELBO, autoguide\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import softplus\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import seaborn as sns\n",
    "import muon as mu\n",
    "import anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45e0d5bd-c307-4b4d-9b9d-ef46a02dc580",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:01:25.387204Z",
     "start_time": "2024-06-01T08:01:20.058306Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/venvs/deeplife/lib/python3.12/site-packages/anndata/_core/anndata.py:1820: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "# dir=\"/scratch/deeplife/\"\n",
    "dir=\"data/\"\n",
    "pbmc = sc.read_10x_h5(dir+\"5k_pbmc_protein_v3_nextgem_filtered_feature_bc_matrix.h5\", gex_only=False)\n",
    "pbmc.var_names_make_unique()\n",
    "pbmc.layers[\"counts\"] = pbmc.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c95a25a-bc4d-43ca-9a2e-bb4f45d1ed1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:01:26.223901Z",
     "start_time": "2024-06-01T08:01:25.368144Z"
    }
   },
   "outputs": [],
   "source": [
    "protein = pbmc[:, pbmc.var[\"feature_types\"] == \"Antibody Capture\"].copy()\n",
    "rna = pbmc[:, pbmc.var[\"feature_types\"] == \"Gene Expression\"].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MOFA(PyroModule):\n",
    "    def __init__(self, Ys: dict[str, torch.Tensor], K, batch_size=False, num_iterations=4000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        self.Ys = Ys  # data/observations\n",
    "        self.K = K  # number of factors \n",
    "        \n",
    "        # assert sample dim same in Ys\n",
    "        num_samples = set(Y.shape[0] for Y in self.Ys.values())\n",
    "        assert len(num_samples) == 1\n",
    "        self.num_samples = next(iter(num_samples))\n",
    "        self.num_features = {k: v.shape[1] for k, v in self.Ys.items()}\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K) \n",
    "        \n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\" Creates the model.\n",
    "        \n",
    "        The model needs to be created repeatedly (not sure why), in any case, it is important now, when using \n",
    "        `subsample_size` batch size to subsample the dataset differently in each train iteration\n",
    "        \"\"\"\n",
    "        \n",
    "        # needs to be shared, so returns the same indices in one train step\n",
    "        sample_plate = pyro.plate(\"sample\", self.num_samples, subsample_size=self.batch_size)\n",
    "        # the plates get assigned a dim, depending on when in the plate hierarchy they are used. Unfortunately we want to use\n",
    "        #   feature plates once outside and once inside other plate (sample resp. latent_factor plates, see below)\n",
    "        #   we therefore need to create separate plates for each of those usages\n",
    "        get_feature_plates = lambda dim: {k: pyro.plate(f\"feature_{k}_{dim}\", num_feats) for k, num_feats in self.num_features.items()}\n",
    "\n",
    "        # W matrices for each modality\n",
    "        Ws = {}\n",
    "    \n",
    "        # for each modality create W matrix and alpha vectors\n",
    "        for m, feature_plate in get_feature_plates(-2).items():\n",
    "            # the actual dimensions obtained by plates are read from right to left/inner to outer\n",
    "            with self.latent_factor_plate:\n",
    "                # Sample alphas (controls narrowness of weight distr for each factor) from a Gamma distribution\n",
    "                # Gamma parametrization k, theta or eq. a, b; (where k=a and theta=1/b) \n",
    "                # (if k integer) Gamma = the sum of k independent exponentially distributed random variables, each of \n",
    "                # which has a mean of theta\n",
    "                alpha = pyro.sample(f\"alpha_{m}\", pyro.distributions.Gamma(1, 1))\n",
    "                \n",
    "                with feature_plate:\n",
    "                    # sample weight matrix with Normal prior distribution with alpha narrowness\n",
    "                    Ws[m] = pyro.sample(f\"W_{m}\", pyro.distributions.Normal(0., 1. / alpha))                \n",
    "                \n",
    "        # create Z matrix\n",
    "        # (the actual dimensions are read from right to left/inner to outer)\n",
    "        with self.latent_factor_plate, sample_plate:\n",
    "            # sample factor matrix with Normal prior distribution\n",
    "            Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "    \n",
    "        # estimate for Y\n",
    "        Y_hats = {k: torch.matmul(Z, W.t()) for k, W in Ws.items()}\n",
    "        \n",
    "        for m, feature_plate in get_feature_plates(-1).items():\n",
    "            with feature_plate:\n",
    "                # sample scale (tau) parameter for each feature-~~sample~~ pair with LogNormal prior (has to be positive)\n",
    "                scale_tau = pyro.sample(f\"scale_{m}\", pyro.distributions.LogNormal(0., 1.))\n",
    "                \n",
    "                with sample_plate as sub_indices:\n",
    "                    Y, Y_hat = self.Ys[m][sub_indices], Y_hats[m]\n",
    "                    \n",
    "                    # masking the NA values such that they are not considered in the distributions\n",
    "                    obs_mask = torch.logical_not(torch.isnan(Y))\n",
    "                    \n",
    "                    with pyro.poutine.mask(mask=obs_mask):\n",
    "                        # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                        Y = torch.nan_to_num(Y, nan=0) \n",
    "                \n",
    "                        # # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                        # scale = pyro.sample(\"scale\", pyro.distributions.LogNormal(0., 1.))\n",
    "                        \n",
    "                        # compare sampled estimation to the true observation Y\n",
    "                        pyro.sample(f\"obs_{m}\", pyro.distributions.Normal(Y_hat, scale_tau), obs=Y)\n",
    "                        # pyro.sample(\"obs\", pyro.distributions.NegativeBinomial(Y_hat, scale_tau), obs=Y)\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        guide = autoguide.AutoDelta(self.model)\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        train_loss = []\n",
    "        for j in range(self.num_iterations):\n",
    "            # calculate the loss and take a gradient step\n",
    "            # (loss should be already scaled down by the subsample_size)\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.num_samples)\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.num_samples))\n",
    "        \n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        # map_estimates = guide(self.Y)  # not sure why needed Y?\n",
    "        # \"Note that Pyro enforces that model() and guide() have the same call signature, i.e. both callables should take the same arguments.\"\n",
    "        map_estimates = guide()\n",
    "        \n",
    "        return train_loss, map_estimates, guide\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T08:01:26.294524Z",
     "start_time": "2024-06-01T08:01:26.207710Z"
    }
   },
   "id": "215ca913ab3c58b1",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b384fdc-4864-44e3-bffd-c7ca858d52a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:46:53.288286Z",
     "start_time": "2024-06-01T07:40:36.738842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 1544855.6143\n",
      "[iteration 0201] loss: 43426.8338\n",
      "[iteration 0401] loss: -19751.4866\n",
      "[iteration 0601] loss: -33614.6363\n",
      "[iteration 0801] loss: -48270.5133\n",
      "[iteration 1001] loss: -56885.2889\n",
      "[iteration 1201] loss: -42953.5086\n",
      "[iteration 1401] loss: -61820.0962\n",
      "[iteration 1601] loss: -54214.0418\n",
      "[iteration 1801] loss: -70193.2789\n",
      "[iteration 2001] loss: -64776.4558\n",
      "[iteration 2201] loss: -66183.7409\n",
      "[iteration 2401] loss: -45832.1790\n",
      "[iteration 2601] loss: -69544.2023\n",
      "[iteration 2801] loss: -67679.5133\n",
      "[iteration 3001] loss: -69013.3838\n",
      "[iteration 3201] loss: -76465.5619\n",
      "[iteration 3401] loss: -29830.0084\n",
      "[iteration 3601] loss: -71609.8842\n",
      "[iteration 3801] loss: -54700.3288\n"
     ]
    }
   ],
   "source": [
    "mofa = MOFA({\n",
    "    'rna': torch.tensor(rna.X.toarray()),\n",
    "    'protein': torch.tensor(protein.X.toarray()),\n",
    "}, K=5, batch_size=33, num_iterations=4000)\n",
    "loss, map_estimates, trained_guide = mofa.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'alpha_rna': tensor([ 0.6805,  0.7047, 51.2332,  6.3386, 44.6454],\n        grad_fn=<ExpandBackward0>),\n 'W_rna': tensor([[ 5.7743e-16,  4.8005e-16,  2.3599e-16,  2.2752e-17,  1.7289e-15],\n         [ 1.5914e-04,  2.6798e-04,  1.2638e-03,  1.9229e-03,  1.1706e-03],\n         [-7.3769e-06, -6.1725e-06,  9.5526e-06, -1.1013e-05,  1.0729e-05],\n         ...,\n         [-1.4335e-01,  5.1932e-02, -1.5062e-02,  6.9615e-02,  2.6003e-02],\n         [ 7.9154e-22, -3.3275e-22, -7.4161e-23, -4.1880e-22,  1.9829e-22],\n         [-1.4943e-03, -2.7019e-03, -7.7028e-03, -8.6110e-04,  1.1006e-02]],\n        grad_fn=<ExpandBackward0>),\n 'alpha_protein': tensor([0.3126, 0.5185, 1.0603, 0.4464, 0.7072], grad_fn=<ExpandBackward0>),\n 'W_protein': tensor([[-2.5605, -0.4950, -0.4921, -2.5739,  0.4239],\n         [-4.4372, -1.8422,  0.8739, -2.3409,  1.4852],\n         [-2.8029, -0.1755, -0.7576, -1.4559,  0.1247],\n         [-1.8529, -0.6747,  0.1978, -2.7917,  2.1802],\n         [-2.6411, -0.4595, -0.2467, -2.2207,  1.6468],\n         [-2.0921, -0.3927, -1.3260, -2.6907,  1.4632],\n         [-2.4458, -0.3391, -0.6757, -1.8404,  1.4905],\n         [-2.5404, -0.1281, -0.7828, -1.2156,  0.4404],\n         [-3.1837,  0.0571, -1.9587, -2.0602,  1.7360],\n         [-3.7173, -0.3593, -1.3647, -2.4394,  0.8677],\n         [-3.1601, -0.9921,  0.5236, -1.9675,  1.8245],\n         [-3.9676, -0.6616, -0.5271, -1.8879,  2.1845],\n         [-0.7695,  0.4669, -0.0296,  0.2245, -0.1449],\n         [-1.7902, -0.5411, -0.1056, -1.8654,  0.8266],\n         [-2.9189, -0.4928, -0.8341, -2.6982,  2.4374],\n         [-2.0873,  0.7162, -0.4299, -1.3707,  1.3460],\n         [-4.2247, -0.2061, -1.6288, -3.5030,  1.2944],\n         [-3.9960,  0.4592,  0.4770, -2.5317,  1.1247],\n         [-3.9411,  4.6112, -0.5715, -1.9992, -0.4473],\n         [-2.9915, -1.6268, -0.6684, -2.4151,  1.7146],\n         [-3.3914, -0.2744, -0.4954, -2.1764,  2.4196],\n         [-4.3946,  4.8948, -1.1573, -1.9360, -0.2797],\n         [-2.8421,  0.8616, -1.1415, -2.7973,  0.8898],\n         [-3.0801,  2.5060, -1.1540, -1.8546,  0.2760],\n         [-4.6037, -0.0216, -1.0194, -3.4494,  2.1335],\n         [-2.9171,  2.5548, -1.0037, -1.8388,  1.3215],\n         [-2.4407, -0.5446, -1.2216, -2.3167,  1.1079],\n         [-2.9687, -0.9128, -0.8766, -3.0775,  2.6734],\n         [-4.7278,  5.1176, -0.6608, -1.6910, -0.3320],\n         [-2.5713,  2.4029, -0.9100, -1.3361, -0.0510],\n         [-3.1400,  2.8433, -0.9514, -1.3096, -0.3444],\n         [-2.5988,  2.6037, -1.2342, -2.0546,  0.1759]],\n        grad_fn=<ExpandBackward0>),\n 'Z': tensor([[-1.0713e-01,  7.0127e-03,  7.6328e-02, -1.6219e-01,  1.1213e-01],\n         [ 1.8984e-01,  2.4890e-01, -1.7269e-01,  2.1753e-01, -2.4389e-01],\n         [ 2.1901e-02,  7.2665e-02, -2.9037e-02,  8.1809e-02, -6.7625e-02],\n         [ 3.9313e-02,  7.7193e-02, -2.8005e-02, -6.0252e-02,  3.2980e-02],\n         [-3.9871e-02,  2.6358e-02, -2.7851e-02,  2.4240e-03, -9.9758e-03],\n         [-1.8107e-01,  5.0863e-02, -3.6199e-02,  3.6990e-02, -3.2992e-02],\n         [-1.6250e-01,  6.7188e-02,  4.2626e-02, -6.5415e-02,  5.9783e-02],\n         [-1.3452e-01, -1.6851e-03,  9.3020e-02,  8.0518e-02, -7.1117e-02],\n         [-7.7569e-02,  4.7234e-02,  1.8620e-02, -5.1026e-03,  2.5814e-02],\n         [-2.0534e-01, -1.6958e-01,  2.0977e-02,  2.8226e-02,  1.1225e-02],\n         [ 8.6922e-02,  5.3467e-01, -1.6903e-01,  1.9153e-01, -1.5043e-01],\n         [-4.6033e-02,  3.7654e-02,  3.4053e-03,  6.1790e-04,  7.3261e-03],\n         [-2.0224e-02,  5.4566e-02, -1.1273e-02, -9.6625e-03, -2.7974e-03],\n         [ 8.2636e-04,  2.7204e-02, -7.0535e-03, -4.4445e-03, -4.3156e-03],\n         [ 4.1086e-02,  2.9203e-02, -3.7460e-02, -6.0198e-02,  2.9683e-02],\n         [-3.6360e-02,  2.0442e-02, -6.0192e-03,  6.5256e-03, -1.0430e-02],\n         [ 1.2713e-01,  3.4352e-01, -3.6936e-01,  4.4213e-01, -3.2586e-01],\n         [-8.5512e-02,  7.7268e-02,  1.3225e-03, -1.4451e-02, -9.0200e-05],\n         [-6.1856e-03, -1.6718e-02,  2.5056e-03,  1.9756e-03, -4.1001e-03],\n         [-9.3174e-02,  7.1235e-02, -1.6815e-02, -1.4621e-02, -1.9871e-02],\n         [-4.6205e-02,  6.6643e-02, -2.0985e-02, -5.3374e-03,  2.6812e-02],\n         [ 1.3365e-01,  2.4942e-01, -2.9830e-01,  2.0188e-01, -1.9728e-01],\n         [-6.1340e-02,  5.8990e-02,  6.8738e-03, -6.9144e-04,  1.2808e-02],\n         [ 1.3228e-01, -1.3408e-01, -5.1189e-03, -2.5643e-02,  1.6193e-02],\n         [-1.3678e-01, -2.8357e-03,  2.8588e-02, -1.6806e-02,  3.2416e-02],\n         [-1.2781e-01,  8.9258e-02,  3.7773e-02, -4.9817e-02,  1.0324e-01],\n         [ 7.0845e-04,  8.1801e-04, -2.6733e-03, -1.5329e-03,  9.8063e-04],\n         [-6.2881e-02,  2.1011e-02, -3.7770e-02,  7.2566e-03, -3.2010e-03],\n         [-3.5484e-02,  2.1893e-02, -1.3922e-03,  7.9281e-03, -2.5601e-03],\n         [-1.1817e-01,  9.9977e-02, -2.2104e-02,  1.7716e-02, -7.5106e-03],\n         [-1.5267e-02,  7.0167e-02, -2.6893e-02,  1.0071e-01, -5.4947e-02],\n         [-1.4033e-01,  2.9534e-01, -4.3915e-02, -4.6150e-03, -7.2845e-02],\n         [ 9.0574e-02,  2.9819e-01, -5.2917e-02,  3.3897e-02, -3.3843e-02]],\n        grad_fn=<ExpandBackward0>),\n 'scale_rna': tensor([0.0024, 0.0040, 0.0006,  ..., 0.1521, 0.0008, 0.0173],\n        grad_fn=<ExpandBackward0>),\n 'scale_protein': tensor([61.4854, 72.2264, 58.2279, 63.9345, 81.1319, 28.2433, 33.2099, 89.7395,\n         23.4209, 24.1401, 52.3762, 74.1119,  0.4807, 82.4488, 49.1028, 68.6456,\n         49.4346, 30.7254,  5.9522, 49.9956, 37.7241,  8.7760, 22.8026,  9.9223,\n         59.2376, 26.1515, 21.3816, 41.0663, 16.2987, 10.5838, 10.1110, 10.3787],\n        grad_fn=<ExpandBackward0>)}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_estimates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T07:46:53.317610Z",
     "start_time": "2024-06-01T07:46:53.297148Z"
    }
   },
   "id": "10f723678a643d64",
   "execution_count": 60
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
